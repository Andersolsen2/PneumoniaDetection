{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d320f90f432c331afea02ef66fcddb59448ea200",
    "colab_type": "text",
    "id": "y-HPCpMoyzz-"
   },
   "source": [
    "# Approach\n",
    "\n",
    "* Firstly a convolutional neural network is used to segment the image, using the bounding boxes directly as a mask. \n",
    "* Secondly connected components is used to separate multiple nodules.\n",
    "* Finally a bounding box is simply drawn around every connected component.\n",
    "\n",
    "# Network\n",
    "\n",
    "* The network consists of a number of residual blocks with convolutions and downsampling blocks with max pooling.\n",
    "* At the end of the network a single upsampling layer converts the output to the same shape as the input.\n",
    "\n",
    "As the input to the network is 256 by 256 (instead of the original 1024 by 1024) and the network downsamples a number of times without any meaningful upsampling (the final upsampling is just to match in 256 by 256 mask) the final prediction is very crude. If the network downsamples 4 times the final bounding boxes can only change with at least 16 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13287,
     "status": "ok",
     "timestamp": 1544965140666,
     "user": {
      "displayName": "Cecilie Andre",
      "photoUrl": "",
      "userId": "02719411708897168281"
     },
     "user_tz": -60
    },
    "id": "Bf20FdPAyzz_",
    "outputId": "d0feab87-d0f2-4d3b-a7a0-90d49cabbeee"
   },
   "outputs": [],
   "source": [
    "!pip install pydicom\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import measure\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1544965341714,
     "user": {
      "displayName": "Cecilie Andre",
      "photoUrl": "",
      "userId": "02719411708897168281"
     },
     "user_tz": -60
    },
    "id": "07ClHmU00G93",
    "outputId": "b83cd08d-0021-4dfe-c5fa-624608cd59cb"
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd() \n",
    "print(cwd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8d58cefae21c951e077c02c1a989d020f18465cc",
    "colab_type": "text",
    "id": "LYKrWrEryz0D"
   },
   "source": [
    "# Load ground truth locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e08496a85ef9b0823595c3745d2677c6e84b6a3a",
    "colab": {},
    "colab_type": "code",
    "id": "EItl3H1Myz0F"
   },
   "outputs": [],
   "source": [
    "# empty dictionary\n",
    "nodule_locations = {}\n",
    "# load table\n",
    "with open(os.path.join('./stage_1_train_labels.csv'), mode='r') as infile:\n",
    "    # open reader\n",
    "    reader = csv.reader(infile)\n",
    "    # skip header\n",
    "    next(reader, None)\n",
    "    # loop through rows\n",
    "    for rows in reader:\n",
    "        # retrieve information\n",
    "        filename = rows[0]\n",
    "        location = rows[1:5]\n",
    "        sick = rows[5]\n",
    "        # if row contains a nodule add label to dictionary\n",
    "        # which contains a list of nodule locations per filename\n",
    "        if sick == '1':\n",
    "            # convert string to float to int\n",
    "            location = [int(float(i)) for i in location]\n",
    "            # save nodule location in dictionary\n",
    "            if filename in nodule_locations:\n",
    "                nodule_locations[filename].append(location)\n",
    "            else:\n",
    "                nodule_locations[filename] = [location]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a4fa6da9833fd7cbd476d19584ba35695b38ddb",
    "colab_type": "text",
    "id": "-xn_tPk4yz0I"
   },
   "source": [
    "# Load filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ccd0b0d52cafd125558ed5560a9cc8fa15760bc5",
    "colab": {},
    "colab_type": "code",
    "id": "eyexoFRPyz0J"
   },
   "outputs": [],
   "source": [
    "# load and shuffle filenames\n",
    "folder = './stage_1_train_images'\n",
    "filenames = os.listdir(folder)\n",
    "random.seed(42)\n",
    "random.shuffle(filenames)\n",
    "# split into train and validation filenames\n",
    "n_valid_samples = 1500\n",
    "train_filenames = filenames[n_valid_samples:]\n",
    "valid_filenames = filenames[:n_valid_samples]\n",
    "print('n train samples', len(train_filenames))\n",
    "print('n valid samples', len(valid_filenames))\n",
    "n_train_samples = len(filenames) - n_valid_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "276b80f59fa9acdfd9307d74cee5f705cd6aa5b6",
    "colab_type": "text",
    "collapsed": true,
    "id": "8Mg0jJyYyz0U"
   },
   "source": [
    " # Data generator\n",
    "\n",
    "The dataset is too large to fit into memory, so a generator is made that loads data on the fly.\n",
    "\n",
    "* The generator takes in some filenames, batch_size and other parameters.\n",
    "\n",
    "* The generator outputs a random batch of numpy images and numpy masks.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86b3f780a03cddda78c6adfde461d6ff8dad5672",
    "colab": {},
    "colab_type": "code",
    "id": "MuYx-x-Yyz0V"
   },
   "outputs": [],
   "source": [
    "class generator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, folder, filenames, nodule_locations=None, batch_size=32, image_size=512, shuffle=True, predict=False, augment = False):\n",
    "        self.folder = folder\n",
    "        self.filenames = filenames\n",
    "        self.nodule_locations = nodule_locations\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "        self.shuffle = shuffle\n",
    "        self.predict = predict\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __load__(self, filename):\n",
    "        # load dicom file as numpy array\n",
    "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
    "        # create empty mask\n",
    "        msk = np.zeros(img.shape)\n",
    "        # get filename without extension\n",
    "        filename = filename.split('.')[0]\n",
    "        # if image contains nodules\n",
    "        if filename in nodule_locations:\n",
    "            # loop through nodules\n",
    "            for location in nodule_locations[filename]:\n",
    "                # add 1's at the location of the nodule\n",
    "                x, y, w, h = location\n",
    "                msk[y:y+h, x:x+w] = 1\n",
    "        # resize both image and mask\n",
    "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
    "        msk = resize(msk, (self.image_size, self.image_size), mode='reflect') > 0.5\n",
    "        # if augment then horizontal flip half the time\n",
    "        if self.augment and random.random() > 0.5:\n",
    "            img = np.fliplr(img)\n",
    "            msk = np.fliplr(msk)\n",
    "        # add trailing channel dimension\n",
    "        img = np.expand_dims(img, -1)\n",
    "        msk = np.expand_dims(msk, -1)\n",
    "        return img, msk\n",
    "    \n",
    "    def __loadpredict__(self, filename):\n",
    "        # load dicom file as numpy array\n",
    "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
    "        # resize image\n",
    "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
    "        # add trailing channel dimension\n",
    "        img = np.expand_dims(img, -1)\n",
    "        return img\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # select batch\n",
    "        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # predict mode: return images and filenames\n",
    "        if self.predict:\n",
    "            # load files\n",
    "            imgs = [self.__loadpredict__(filename) for filename in filenames]\n",
    "            # create numpy batch\n",
    "            imgs = np.array(imgs)\n",
    "            return imgs, filenames\n",
    "        # train mode: return images and masks\n",
    "        else:\n",
    "            # load files\n",
    "            items = [self.__load__(filename) for filename in filenames]\n",
    "            # unzip images and masks\n",
    "            imgs, msks = zip(*items)\n",
    "            # create numpy batch\n",
    "            imgs = np.array(imgs)\n",
    "            msks = np.array(msks)\n",
    "            return imgs, msks\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.filenames)\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.predict:\n",
    "            # return everything\n",
    "            return int(np.ceil(len(self.filenames) / self.batch_size))\n",
    "        else:\n",
    "            # return full batches only\n",
    "            return int(len(self.filenames) / self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ebc622a4b406354cc4ef28801eab72346a724d8b",
    "colab_type": "text",
    "id": "XpTAH-62yz0Y"
   },
   "source": [
    "# Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9fc2b108689637a6037b48ebab3f7659b8704bf9",
    "colab": {},
    "colab_type": "code",
    "id": "hj2EXwbTyz0Z"
   },
   "outputs": [],
   "source": [
    "def dense_factor(channels, inputs, size=1):\n",
    "    h_1 = keras.layers.BatchNormalization()(inputs)\n",
    "    h_1 = keras.layers.Conv2D(channels, size, padding='same')(h_1) #, use_bias=False)(h_1)\n",
    "    output = keras.layers.ReLU()(h_1)\n",
    "    return output\n",
    "  \n",
    "\n",
    "def dense_block(channels, inputs, length):\n",
    "\n",
    "    x = inputs\n",
    "    channels_input = channels\n",
    "    \n",
    "    for i in range(length):\n",
    "        \n",
    "        x_1 = dense_factor(channels, x,size=1)\n",
    "        \n",
    "        #x_2 = keras.layers.concatenate([x, x_1], axis=3)\n",
    "        x_2  = keras.layers.add([x, x_1])\n",
    "        \n",
    "        channels_input = channels_input + channels\n",
    "        \n",
    "        x_2 = dense_factor(channels_input, x_2, size=3)\n",
    "        \n",
    "        #concatenated_inputs = keras.layers.concatenate([x, x_1, x_2], axis=3)\n",
    "        concatenated_inputs  = keras.layers.add([x, x_1, x_2])\n",
    "        \n",
    "        #channels_output = channels_input + channels\n",
    "        channels_output = channels \n",
    "    return concatenated_inputs, channels_output\n",
    "  \n",
    "#from tensorflow.python.keras.layers import average_pooling3d\n",
    "\n",
    "from tensorflow.python.layers import pooling\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_network(input_size, channels):\n",
    "    # input\n",
    "    inputs = keras.Input(shape=(input_size, input_size, 1))\n",
    "    x = keras.layers.Conv2D(channels, 7, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')(x)\n",
    "    # x = keras.layers.LeakyReLU(0)(x)\n",
    "    # residual blocks\n",
    "    \n",
    "    # First block\n",
    "    x , channels = dense_block(channels, x, 2)\n",
    "    x  = create_downsample(channels, x)\n",
    "    \n",
    "    # Second block\n",
    "    x , channels = dense_block(channels, x, 2)\n",
    "    x = create_downsample(channels, x)\n",
    "    \n",
    "    # Third block\n",
    "    x , channels= dense_block(channels, x, 2)\n",
    "    x = create_downsample(channels, x)\n",
    "    \n",
    "    # Fourth block\n",
    "    x , channels= dense_block(channels, x, 2)\n",
    "\n",
    "    \n",
    "    \n",
    "    # output\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = pooling.AveragePooling2D(pool_size=(2, 2), strides=2, padding='valid')(x)\n",
    "    x = keras.layers.Conv2D(1, 1, activation='sigmoid')(x)\n",
    "    outputs = keras.layers.UpSampling2D(64)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    " \n",
    "\n",
    "    return model\n",
    "\n",
    "  \n",
    "def mean_iou(y_true, y_pred):\n",
    "    y_pred = tf.round(y_pred)\n",
    "    intersect = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
    "    smooth = tf.ones(tf.shape(intersect))\n",
    "    return tf.reduce_mean((intersect + smooth) / (union - intersect + smooth))\n",
    "\n",
    "\n",
    "  \n",
    "# create network and compiler\n",
    "model = create_network(input_size=1024, channels=1)\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=.001),loss=keras.losses.binary_crossentropy,metrics=['accuracy', mean_iou])  \n",
    "print( model.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bee3bc9363e9c1829eadf17da7a67fbd1a6a369e",
    "colab_type": "text",
    "id": "jLsIVUCGyz0d"
   },
   "source": [
    "# Train network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4369be30f61440eb6858d57829fa541c4ee893bf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7224130,
     "status": "error",
     "timestamp": 1544897251651,
     "user": {
      "displayName": "Cecilie Andre",
      "photoUrl": "",
      "userId": "02719411708897168281"
     },
     "user_tz": -60
    },
    "id": "j8zRCoHzyz0e",
    "outputId": "2dadacec-e0f2-444d-c222-a1b552a57b62",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# weights\n",
    "model_weight = './weights-improvement-half-05-0.97.h5'\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Save the model\n",
    "filepath=\"weights-improvement-half-{epoch:02d}-{val_acc:.2f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=False, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# create train and validation generators\n",
    "folder = './stage_1_train_images'\n",
    "train_gen = generator(folder, train_filenames, nodule_locations, batch_size=32, image_size=256, shuffle=True, augment=True, predict=False)\n",
    "valid_gen = generator(folder, valid_filenames, nodule_locations, batch_size=32, image_size=256, shuffle=False, predict=False)\n",
    "\n",
    "model.load_weights(model_weight)\n",
    "history = model.fit_generator(train_gen, validation_data=valid_gen, epochs=50, callbacks=callbacks_list, shuffle=True, verbose=1, initial_epoch=1)\n",
    "\n",
    "\n",
    "#plt.figure(figsize=(12,4))\n",
    "#plt.subplot(131)\n",
    "#plt.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "#plt.plot(history.epoch, history.history[\"val_loss\"], label=\"Valid loss\")\n",
    "#plt.legend()\n",
    "#plt.subplot(132)\n",
    "#plt.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
    "#plt.plot(history.epoch, history.history[\"val_acc\"], label=\"Valid accuracy\")\n",
    "#plt.legend()\n",
    "#plt.subplot(133)\n",
    "#plt.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train iou\")\n",
    "#plt.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Valid iou\")\n",
    "#plt.legend()\n",
    "#plt.savefig('loss.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "25dc7ad848190bb37349a80f96ed2bdb5c3821b0",
    "colab_type": "text",
    "id": "JkjCSA7Xyz0n"
   },
   "source": [
    "# Predict test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1217,
     "status": "error",
     "timestamp": 1544929631221,
     "user": {
      "displayName": "Cecilie Andre",
      "photoUrl": "",
      "userId": "02719411708897168281"
     },
     "user_tz": -60
    },
    "id": "Rw9FPoScV8fk",
    "outputId": "8ea3a95c-9d83-4afb-8e04-d31d99f7365b"
   },
   "outputs": [],
   "source": [
    "# empty dictionary\n",
    "nodule_locations_new = {}\n",
    "# load table\n",
    "with open(os.path.join('./stage_1_train_labels.csv'), mode='r') as infile:\n",
    "    # open reader\n",
    "    reader = csv.reader(infile)\n",
    "    # skip header\n",
    "    next(reader, None)\n",
    "    # loop through rows\n",
    "    for rows in reader:\n",
    "        # retrieve information\n",
    "        filename = rows[0]\n",
    "        location = rows[1:5]\n",
    "        nodule = rows[5]\n",
    "        # if row contains a nodule add label to dictionary\n",
    "        # which contains a list of nodule locations per filename\n",
    "        if nodule == '1':\n",
    "            # convert string to float to int\n",
    "            location = [int(float(i)) for i in location]\n",
    "            # save nodule location in dictionary\n",
    "            if filename in nodule_locations_new:\n",
    "                nodule_locations_new[filename].append(location)\n",
    "            else:\n",
    "                nodule_locations_new[filename] = [location]\n",
    "        else:\n",
    "            location = [0,0,0,0]\n",
    "            nodule_locations_new[filename] = [location]\n",
    "\n",
    "\n",
    "\n",
    "# load and shuffle filenames\n",
    "folder = './stage_1_train_images'\n",
    "\n",
    "val_gen = generator(folder, valid_filenames, None, batch_size=32, image_size=256, shuffle=False, predict=True)\n",
    "\n",
    "# loop through valset\n",
    "c = 0\n",
    "stroke=0\n",
    "rgb = [255, 251, 204] \n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(251, 251))\n",
    "for imgs, filename in val_gen:\n",
    "    if c in range(3):\n",
    "      \n",
    "      # predict batch of images\n",
    "      pred = model.predict(imgs)\n",
    "      truestring= filename[0]\n",
    "      filename = str(truestring[:-4])\n",
    "      pred = pred[0,:,:,:]\n",
    "      #print(pred.shape)\n",
    "      imgs = imgs[0,:,:,:]\n",
    "      pred = resize(pred, (1024, 1024), mode='reflect')\n",
    "      imgs = resize(imgs, (1024, 1024), mode='reflect')\n",
    "      imgs = imgs[:,:,0]\n",
    "      #print(imgs.shape)\n",
    "      # threshold predicted mask\n",
    "      comp1 = pred[:, :, 0] > 0.5\n",
    "      # apply connected components\n",
    "      comp2 = measure.label(comp1)\n",
    "      #print(comp2.shape)\n",
    "      # Heat map      \n",
    "      plt.subplot(3, 2, 2*c + 1)\n",
    "      imgstru = imgs\n",
    "      imgstru = np.stack([imgstru] * 3, axis=2)\n",
    "      plt.imshow(imgstru, cmap=plt.cm.gist_gray)\n",
    "\n",
    "      for location in nodule_locations_new[filename]:\n",
    "            x1, y1, width, height = location \n",
    "            if x1!=0 and y1!=0 and width!=0 and height!=0:\n",
    "              y1 = y1 - height/2\n",
    "              x1 = x1 - width/2\n",
    "\n",
    "              y1 = int(y1)\n",
    "              x1 = int(x1)\n",
    "              y2 = int(y2)\n",
    "              x2 = int(x2) \n",
    "              rectangle_gap = plt.Rectangle((x1,y1),width,height,linewidth=5,edgecolor='r',fill = False, facecolor='none')\n",
    "              plt.gca().add_patch(rectangle_gap)\n",
    "\n",
    "              #print('true',y1,x1)\n",
    "              #imgstru[y1:y1 + stroke, x1:x2] = rgb\n",
    "              #imgstru[y2:y2 + stroke, x1:x2] = rgb\n",
    "              #imgstru[y1:y2, x1:x1 + stroke] = rgb\n",
    "              #imgstru[y1:y2, x2:x2 + stroke] = rgb\n",
    "              #print(x1, y1,height,width)\n",
    "            else:\n",
    "              continue\n",
    " \n",
    "      plt.axis('off') \n",
    "      con = []\n",
    "      plt.subplot(3, 2, 2*c + 2)\n",
    "      imgsval = imgs\n",
    "      imgsval = np.stack([imgsval] * 3, axis=2)\n",
    "      plt.imshow(imgsval, cmap=plt.cm.gist_gray)\n",
    "\n",
    "      for region in measure.regionprops(comp2):\n",
    "            print(region)\n",
    "            # retrieve x, y, height and width\n",
    "            y1, x1, y2, x2 = region.bbox\n",
    "            h = int(y2 - y1)\n",
    "            w = int(x2 - x1)\n",
    "            y1 = int(y1)\n",
    "            x1 = int(x1)\n",
    "    \n",
    "            #print('val',y1,x1)\n",
    "            print(x1, y1, w, h, y2, x2)\n",
    "\n",
    "            # proxy for confidence score\n",
    "            #conf = np.mean(pred[y:y+height, x:x+width])\n",
    "            rectangle_gap = plt.Rectangle((x1,y1),w,h,linewidth=5,edgecolor='r',fill = False,facecolor='none')\n",
    "            plt.gca().add_patch(rectangle_gap)              \n",
    "            #imgsval[y1:y1 + stroke, x1:x2] = rgb\n",
    "            #imgsval[y2:y2 + stroke, x1:x2] = rgb\n",
    "            #imgsval[y1:y2, x1:x1 + stroke] = rgb\n",
    "            #imgsval[y1:y2, x2:x2 + stroke] = rgb\n",
    "            #con.append(conf)\n",
    "      plt.axis('off') \n",
    "    else:\n",
    "      break\n",
    "    c+=1\n",
    "    \n",
    "    \n",
    "plt.tight_layout()    \n",
    "plt.show()    \n",
    "fig.savefig('truth vs. predictions1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c9277e4ec9f12712dd690002c540b396278c504",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5709
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16304,
     "status": "error",
     "timestamp": 1544927959337,
     "user": {
      "displayName": "Cecilie Andre",
      "photoUrl": "",
      "userId": "02719411708897168281"
     },
     "user_tz": -60
    },
    "id": "B-L1rkPDyz0o",
    "outputId": "8396345c-6664-493d-f442-0a26e06c227e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load and shuffle filenames\n",
    "folder = './stage_1_test_images'\n",
    "test_filenames = os.listdir(folder)\n",
    "print('n test samples:', len(test_filenames))\n",
    "\n",
    "# create test generator with predict flag set to True\n",
    "test_gen = generator(folder, test_filenames, None, batch_size=25, image_size=256, shuffle=False, predict=True)\n",
    "\n",
    "# create submission dictionary\n",
    "submission_dict = {}\n",
    "# loop through testset\n",
    "for imgs, filenames in test_gen:\n",
    "    # predict batch of images\n",
    "    preds = model.predict(imgs)\n",
    "    print(preds.shape)\n",
    "    # loop through batch\n",
    "    for pred, filename in zip(preds, filenames):\n",
    "        print(pred.shape)\n",
    "\n",
    "        # resize predicted mask\n",
    "        pred = resize(pred, (1024, 1024), mode='reflect')\n",
    "        \n",
    "        print(pred.shape)\n",
    "\n",
    "        # threshold predicted mask\n",
    "        comp = pred[:, :, 0] > 0.5\n",
    "        # apply connected components\n",
    "        comp = measure.label(comp)\n",
    "        # apply bounding boxes\n",
    "        predictionString = ''\n",
    "        for region in measure.regionprops(comp):\n",
    "            # retrieve x, y, height and width\n",
    "            y, x, y2, x2 = region.bbox\n",
    "            height = y2 - y\n",
    "            width = x2 - x\n",
    "            x = int(x + width/2)\n",
    "            y = int(y + height/2)\n",
    "            # proxy for confidence score\n",
    "            conf = np.mean(pred[y:y+height, x:x+width])\n",
    "            # add to predictionString\n",
    "            predictionString += str(conf) + ' ' + str(x) + ' ' + str(y) + ' ' + str(width) + ' ' + str(height) + ' '\n",
    "        # add filename and predictionString to dictionary\n",
    "        filename = filename.split('.')[0]\n",
    "        submission_dict[filename] = predictionString\n",
    "    # stop if we've got them all\n",
    "    if len(submission_dict) >= len(test_filenames):\n",
    "        break\n",
    "\n",
    "# save dictionary as csv file\n",
    "sub = pd.DataFrame.from_dict(submission_dict,orient='index')\n",
    "sub.index.names = ['patientId']\n",
    "sub.columns = ['PredictionString']\n",
    "sub.to_csv('submission1_conf=0.5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a953a21bd4cb1f7a8d7d1bb875272f81cd080be9",
    "colab": {},
    "colab_type": "code",
    "id": "pyO2Kh01yz0r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "chexnet-batch-normalization-hyparameter-tuning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
