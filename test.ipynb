{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test.ipynb","version":"0.3.2","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"accelerator":"GPU"},"cells":[{"metadata":{"_uuid":"d320f90f432c331afea02ef66fcddb59448ea200","colab_type":"text","id":"y-HPCpMoyzz-"},"cell_type":"markdown","source":["# Network\n","\n"]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","colab_type":"code","executionInfo":{"status":"ok","timestamp":1544969658257,"user_tz":-60,"elapsed":6741,"user":{"displayName":"Cecilie Andre","photoUrl":"","userId":"02719411708897168281"}},"id":"Bf20FdPAyzz_","outputId":"12d0eb96-20de-4e9a-caf5-a4b4b2b66b6b","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["!pip install pydicom\n","\n","import os\n","import csv\n","import random\n","import pydicom\n","import numpy as np\n","import pandas as pd\n","from skimage import measure\n","from skimage.transform import resize\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from matplotlib import pyplot as plt"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pydicom in /usr/local/lib/python2.7/dist-packages (1.2.1)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1544969661143,"user_tz":-60,"elapsed":985,"user":{"displayName":"Cecilie Andre","photoUrl":"","userId":"02719411708897168281"}},"id":"07ClHmU00G93","outputId":"520e0450-3d64-4ed5-a39f-137274af82a8","colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["\n","\n","cwd = os.getcwd() \n","print(cwd)\n","\n","os.chdir('/content/drive/My Drive/LUNG/')\n","\n","cwd = os.getcwd() \n","print(cwd)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content\n","/content/drive/My Drive/LUNG\n"],"name":"stdout"}]},{"metadata":{"id":"5-Tb_UY6usYi","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"_uuid":"8d58cefae21c951e077c02c1a989d020f18465cc","colab_type":"text","id":"LYKrWrEryz0D"},"cell_type":"markdown","source":["# Load ground truth locations\n"]},{"metadata":{"_uuid":"e08496a85ef9b0823595c3745d2677c6e84b6a3a","colab_type":"code","id":"EItl3H1Myz0F","colab":{}},"cell_type":"code","source":["# empty dictionary\n","nodule_locations = {}\n","# load table\n","with open(os.path.join('./stage_1_train_labels.csv'), mode='r') as infile:\n","    # open reader\n","    reader = csv.reader(infile)\n","    # skip header\n","    next(reader, None)\n","    # loop through rows\n","    for rows in reader:\n","        # retrieve information\n","        filename = rows[0]\n","        location = rows[1:5]\n","        sick = rows[5]\n","        # if row contains a nodule add label to dictionary\n","        # which contains a list of nodule locations per filename\n","        if sick == '1':\n","            # convert string to float to int\n","            location = [int(float(i)) for i in location]\n","            # save nodule location in dictionary\n","            if filename in nodule_locations:\n","                nodule_locations[filename].append(location)\n","            else:\n","                nodule_locations[filename] = [location]"],"execution_count":0,"outputs":[]},{"metadata":{"_uuid":"5a4fa6da9833fd7cbd476d19584ba35695b38ddb","colab_type":"text","id":"-xn_tPk4yz0I"},"cell_type":"markdown","source":["# Load filenames"]},{"metadata":{"_uuid":"ccd0b0d52cafd125558ed5560a9cc8fa15760bc5","colab_type":"code","id":"eyexoFRPyz0J","outputId":"5c9b356a-0d9f-419f-a34c-d7c51623f41d","executionInfo":{"status":"ok","timestamp":1544969669204,"user_tz":-60,"elapsed":954,"user":{"displayName":"Cecilie Andre","photoUrl":"","userId":"02719411708897168281"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# load and shuffle filenames\n","folder = './stage_1_train_images'\n","filenames = os.listdir(folder)\n","random.seed(42)\n","random.shuffle(filenames)\n","# split into train and validation filenames\n","n_valid_samples = 1500\n","train_filenames = filenames[n_valid_samples:]\n","valid_filenames = filenames[:n_valid_samples]\n","print('n train samples', len(train_filenames))\n","print('n valid samples', len(valid_filenames))\n","n_train_samples = len(filenames) - n_valid_samples"],"execution_count":0,"outputs":[{"output_type":"stream","text":["('n train samples', 24214)\n","('n valid samples', 1500)\n"],"name":"stdout"}]},{"metadata":{"_uuid":"276b80f59fa9acdfd9307d74cee5f705cd6aa5b6","colab_type":"text","collapsed":true,"id":"8Mg0jJyYyz0U"},"cell_type":"markdown","source":[" # Data generator\n","\n","The dataset is too large to fit into memory, so a generator is made that loads data on the fly.\n","\n","* The generator takes in some filenames, batch_size and other parameters.\n","\n","* The generator outputs a random batch of numpy images and numpy masks.\n","    "]},{"metadata":{"_uuid":"86b3f780a03cddda78c6adfde461d6ff8dad5672","colab_type":"code","id":"MuYx-x-Yyz0V","colab":{}},"cell_type":"code","source":["class generator(keras.utils.Sequence):\n","    \n","    def __init__(self, folder, filenames, nodule_locations=None, batch_size=32, image_size=512, shuffle=True, predict=False, augment = False):\n","        self.folder = folder\n","        self.filenames = filenames\n","        self.nodule_locations = nodule_locations\n","        self.batch_size = batch_size\n","        self.image_size = image_size\n","        self.augment = augment\n","        self.shuffle = shuffle\n","        self.predict = predict\n","        self.on_epoch_end()\n","        \n","    def __load__(self, filename):\n","        # load dicom file as numpy array\n","        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n","        # create empty mask\n","        msk = np.zeros(img.shape)\n","        # get filename without extension\n","        filename = filename.split('.')[0]\n","        # if image contains nodules\n","        if filename in nodule_locations:\n","            # loop through nodules\n","            for location in nodule_locations[filename]:\n","                # add 1's at the location of the nodule\n","                x, y, w, h = location\n","                msk[y:y+h, x:x+w] = 1\n","        # resize both image and mask\n","        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n","        msk = resize(msk, (self.image_size, self.image_size), mode='reflect') > 0.5\n","        # if augment then horizontal flip half the time\n","        if self.augment and random.random() > 0.5:\n","            img = np.fliplr(img)\n","            msk = np.fliplr(msk)\n","        # add trailing channel dimension\n","        img = np.expand_dims(img, -1)\n","        msk = np.expand_dims(msk, -1)\n","        return img, msk\n","    \n","    def __loadpredict__(self, filename):\n","        # load dicom file as numpy array\n","        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n","        # resize image\n","        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n","        # add trailing channel dimension\n","        img = np.expand_dims(img, -1)\n","        return img\n","        \n","    def __getitem__(self, index):\n","        # select batch\n","        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n","        # predict mode: return images and filenames\n","        if self.predict:\n","            # load files\n","            imgs = [self.__loadpredict__(filename) for filename in filenames]\n","            # create numpy batch\n","            imgs = np.array(imgs)\n","            return imgs, filenames\n","        # train mode: return images and masks\n","        else:\n","            # load files\n","            items = [self.__load__(filename) for filename in filenames]\n","            # unzip images and masks\n","            imgs, msks = zip(*items)\n","            # create numpy batch\n","            imgs = np.array(imgs)\n","            msks = np.array(msks)\n","            return imgs, msks\n","        \n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            random.shuffle(self.filenames)\n","        \n","    def __len__(self):\n","        if self.predict:\n","            # return everything\n","            return int(np.ceil(len(self.filenames) / self.batch_size))\n","        else:\n","            # return full batches only\n","            return int(len(self.filenames) / self.batch_size)"],"execution_count":0,"outputs":[]},{"metadata":{"_uuid":"ebc622a4b406354cc4ef28801eab72346a724d8b","colab_type":"text","id":"XpTAH-62yz0Y"},"cell_type":"markdown","source":["# Network\n"]},{"metadata":{"_uuid":"9fc2b108689637a6037b48ebab3f7659b8704bf9","colab_type":"code","id":"hj2EXwbTyz0Z","colab":{}},"cell_type":"code","source":["def create_downsample(channels, inputs):\n","    x = keras.layers.BatchNormalization()(inputs)\n","    x = keras.layers.LeakyReLU(0)(x)\n","    x = keras.layers.Conv2D(channels, 1, padding='same', use_bias=False)(x)\n","    x = keras.layers.MaxPool2D(2)(x)\n","    return x\n","\n","def create_resblock(channels, inputs):\n","    x = keras.layers.BatchNormalization()(inputs)\n","    x = keras.layers.LeakyReLU(0)(x)\n","    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n","    x = keras.layers.BatchNormalization()(x)\n","    x = keras.layers.LeakyReLU(0)(x)\n","    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n","    return keras.layers.add([x, inputs])\n","\n","def create_network(input_size, channels, n_blocks=2, depth=5):\n","    # input\n","    inputs = keras.Input(shape=(input_size, input_size, 1))\n","    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(inputs)\n","    # residual blocks\n","    for d in range(depth):\n","        channels = channels * 2\n","        x = create_downsample(channels, x)\n","        for b in range(n_blocks):\n","            x = create_resblock(channels, x)\n","    # output\n","    x = keras.layers.BatchNormalization()(x)\n","    x = keras.layers.LeakyReLU(0)(x)\n","    x = keras.layers.Conv2D(1, 1, activation='sigmoid')(x)\n","    outputs = keras.layers.UpSampling2D(2**depth)(x)\n","    model = keras.Model(inputs=inputs, outputs=outputs)\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"_uuid":"bee3bc9363e9c1829eadf17da7a67fbd1a6a369e","colab_type":"text","id":"jLsIVUCGyz0d"},"cell_type":"markdown","source":["# Train network\n"]},{"metadata":{"_uuid":"4369be30f61440eb6858d57829fa541c4ee893bf","colab_type":"code","executionInfo":{"status":"ok","timestamp":1544969683305,"user_tz":-60,"elapsed":4685,"user":{"displayName":"Cecilie Andre","photoUrl":"","userId":"02719411708897168281"}},"id":"j8zRCoHzyz0e","outputId":"79872bea-adf7-4cb8-bf8c-2f967517d9da","scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# mean iou as a metric\n","def mean_iou(y_true, y_pred):\n","    y_pred = tf.round(y_pred)\n","    intersect = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n","    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n","    smooth = tf.ones(tf.shape(intersect))\n","    return tf.reduce_mean((intersect + smooth) / (union - intersect + smooth))\n","\n","# create network and compiler\n","model = create_network(input_size=256, channels=32, n_blocks=2, depth=4)\n","model.compile(optimizer=keras.optimizers.Adam(lr=.0001),loss=keras.losses.binary_crossentropy,metrics=['accuracy', mean_iou])\n","\n","# weights\n","model_weight = './weights-improvement-22-0.98.h5'\n","from keras.callbacks import ModelCheckpoint\n","\n","# Save the model\n","filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.h5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n","callbacks_list = [checkpoint]\n","\n","# create train and validation generators\n","folder = './stage_1_train_images'\n","train_gen = generator(folder, train_filenames, nodule_locations, batch_size=32, image_size=256, shuffle=True, augment=True, predict=False)\n","valid_gen = generator(folder, valid_filenames, nodule_locations, batch_size=32, image_size=256, shuffle=False, predict=False)\n","\n","model.load_weights(model_weight)\n","history = model.fit_generator(train_gen, validation_data=valid_gen, epochs=50, callbacks=callbacks_list, shuffle=True, verbose=1, initial_epoch=22)\n","\n","\n","plt.figure(figsize=(12,4))\n","plt.subplot(131)\n","plt.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n","plt.plot(history.epoch, history.history[\"val_loss\"], label=\"Valid loss\")\n","plt.legend()\n","plt.subplot(132)\n","plt.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n","plt.plot(history.epoch, history.history[\"val_acc\"], label=\"Valid accuracy\")\n","plt.legend()\n","plt.subplot(133)\n","plt.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train iou\")\n","plt.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Valid iou\")\n","plt.legend()\n","plt.savefig('loss.png')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"_uuid":"25dc7ad848190bb37349a80f96ed2bdb5c3821b0","colab_type":"text","id":"JkjCSA7Xyz0n"},"cell_type":"markdown","source":["# Predict test images"]},{"metadata":{"colab_type":"code","id":"Rw9FPoScV8fk","colab":{}},"cell_type":"code","source":["# empty dictionary\n","nodule_locations_new = {}\n","# load table\n","with open(os.path.join('./stage_1_train_labels.csv'), mode='r') as infile:\n","    # open reader\n","    reader = csv.reader(infile)\n","    # skip header\n","    next(reader, None)\n","    # loop through rows\n","    for rows in reader:\n","        # retrieve information\n","        filename = rows[0]\n","        location = rows[1:5]\n","        nodule = rows[5]\n","        # if row contains a nodule add label to dictionary\n","        # which contains a list of nodule locations per filename\n","        if nodule == '1':\n","            # convert string to float to int\n","            location = [int(float(i)) for i in location]\n","            # save nodule location in dictionary\n","            if filename in nodule_locations_new:\n","                nodule_locations_new[filename].append(location)\n","            else:\n","                nodule_locations_new[filename] = [location]\n","        else:\n","            location = [0,0,0,0]\n","            nodule_locations_new[filename] = [location]\n","\n","\n","\n","# load and shuffle filenames\n","folder = './stage_1_train_images'\n","\n","val_gen = generator(folder, valid_filenames, None, batch_size=32, image_size=256, shuffle=True, predict=True)\n","\n","# loop through valset\n","c = 0\n","stroke=0\n","rgb = [255, 251, 204] \n","\n","\n","fig = plt.figure(figsize=(100, 300))\n","for imgs, filename in val_gen:\n","    if c in range(10):\n","      \n","      # predict batch of images and resize\n","      pred = model.predict(imgs)\n","      truestring= filename[0]\n","      filename = str(truestring[:-4])\n","      pred = pred[0,:,:,:]\n","      imgs = imgs[0,:,:,:]\n","      pred = resize(pred, (1024, 1024), mode='reflect')\n","      imgs = resize(imgs, (1024, 1024), mode='reflect')\n","      imgs = imgs[:,:,0]\n","\n","      # threshold predicted mask\n","      comp1 = pred[:, :, 0] > 0.5\n","      # apply connected components\n","      comp2 = measure.label(comp1)\n","\n","      \n","      \n","      # Heat map      \n","      plt.subplot(10, 2, 2*c + 1)\n","      imgstru = imgs\n","      imgstru = np.stack([imgstru] * 3, axis=2)\n","      plt.imshow(imgstru, cmap=plt.cm.gist_gray)\n","\n","      for location in nodule_locations_new[filename]:\n","            x1, y1, width, height = location \n","            if x1!=0 and y1!=0 and width!=0 and height!=0:\n","  \n","              y1 = int(y1)\n","              x1 = int(x1)\n","\n","              rectangle_gap = plt.Rectangle((x1,y1),width,height,linewidth=10, linestyle = '--', edgecolor='r',fill = False, facecolor='none')\n","              plt.gca().add_patch(rectangle_gap)\n","\n","              continue\n"," \n","      plt.axis('off') \n","      con = []\n","      plt.subplot(10, 2, 2*c + 2)\n","      imgsval = imgs\n","      imgsval = np.stack([imgsval] * 3, axis=2)\n","      plt.imshow(imgsval, cmap=plt.cm.gist_gray)\n","\n","      for region in measure.regionprops(comp2):\n","\n","            # retrieve x, y, height and width\n","            y1, x1, y2, x2 = region.bbox\n","            h = int(y2 - y1)\n","            w = int(x2 - x1)\n","            y1 = int(y1)\n","            x1 = int(x1)\n","\n","            rectangle_gap = plt.Rectangle((x1,y1),w,h,linewidth=10,linestyle = '--', edgecolor='r',fill = False,facecolor='none')\n","            plt.gca().add_patch(rectangle_gap)              \n","\n","      plt.axis('off') \n","    else:\n","      break\n","    c+=1\n","    \n","    \n","plt.tight_layout()    \n","plt.show()    \n","fig.savefig('truth vs. predictions2.jpg')"],"execution_count":0,"outputs":[]},{"metadata":{"_uuid":"2c9277e4ec9f12712dd690002c540b396278c504","colab_type":"code","id":"B-L1rkPDyz0o","scrolled":false,"colab":{}},"cell_type":"code","source":["# load and shuffle filenames\n","folder = './stage_1_test_images'\n","test_filenames = os.listdir(folder)\n","print('n test samples:', len(test_filenames))\n","\n","# create test generator with predict flag set to True\n","test_gen = generator(folder, test_filenames, None, batch_size=25, image_size=256, shuffle=False, predict=True)\n","\n","# create submission dictionary\n","submission_dict = {}\n","# loop through testset\n","for imgs, filenames in test_gen:\n","    # predict batch of images\n","    preds = model.predict(imgs)\n","    #print(preds.shape)\n","    # loop through batch\n","    for pred, filename in zip(preds, filenames):\n","        #print(pred.shape)\n","\n","        # resize predicted mask\n","        pred = resize(pred, (1024, 1024), mode='reflect')\n","        \n","        #print(pred.shape)\n","\n","        # threshold predicted mask\n","        comp = pred[:, :, 0] > 0.7\n","        # apply connected components\n","        comp = measure.label(comp)\n","        # apply bounding boxes\n","        predictionString = ''\n","        for region in measure.regionprops(comp):\n","            # retrieve x, y, height and width\n","            y, x, y2, x2 = region.bbox\n","            height = y2 - y\n","            width = x2 - x\n","\n","            # proxy for confidence score\n","            conf = np.mean(pred[y:y+height, x:x+width])\n","            # add to predictionString\n","            predictionString += str(conf) + ' ' + str(x) + ' ' + str(y) + ' ' + str(width) + ' ' + str(height) + ' '\n","        # add filename and predictionString to dictionary\n","        filename = filename.split('.')[0]\n","        submission_dict[filename] = predictionString\n","    # stop if we've got them all\n","    if len(submission_dict) >= len(test_filenames):\n","        break\n","\n","# save dictionary as csv file\n","sub = pd.DataFrame.from_dict(submission_dict,orient='index')\n","sub.index.names = ['patientId']\n","sub.columns = ['PredictionString']\n","sub.to_csv('submission1_conf=0.7.csv')"],"execution_count":0,"outputs":[]}]}